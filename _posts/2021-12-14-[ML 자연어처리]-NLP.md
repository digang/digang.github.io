---
layout: post
title : "[ML 머신러닝] 자연어처리"
categories: [ML, NLP]
tags: [머신러닝 , 딥러닝, ml , nlp] # TAG 는 소문자로 작성할 것
use_math : true
---

# **NLP?**

- Natural Language Preocessing
- 인간의 언어를 이해하고 생산할 수 있도록 한, 딥러닝을 이용한 방법

---

## **NLP의 분야**

자연어 처리 분야에서는 예전부터 `NLU`에 대한 연구를 많이 했었다. 최근 딥러닝 시대로 넘어오면서 `NLG`에 대한 연구도 활발해 지고 있다고 한다.


|NLG|NLG + NLU|NLU|
|---|---|---|
|- Language Modeling <br> - Article Generation| - Chatbot <br> -Summerization <br> - Question Answering <br> - Machine Transiation |- Text Classification <br> - POS Tagging <br> - Sentiment Analysis <br> - Machine Reading Comprehension <br> - Named Entity Recognition <br> - Sematic Parsing|


좀 더 구조적으로 알아보면, 자연어 처리란(NPL)란 컴퓨터에게 사람처럼 텍스트를 이해시키는 것이 아니다. 문자 언어(written language)에 대한 통계적 구조를 만들어 처리하는 것이다. 자연어 처리에서의 딥러닝은 단어, 문장, 문단에 적용한 패턴들을 인식하는 과정이라고 생각하면 된다.

## **NLP 트렌드**

- 텍스트 데이터는 단어의 시퀀스라고 볼 수 있다. 그리고 각 단어는 Word2Vec, GloVe라는 기술을 통해 벡터로 표현할 수 있다.
- RNN-family models(LSTM, GRU)는 입력으로 단어의 벡터들의 시퀀스로 이루어져있는 NLP의 주요 taskd이다.
- Attention module과 Transformer model을 통해서 NLP의 성능을 전반적으로 상승시킬 수 있다. 이들은 RNN을 self-attention으로 대채하였다.
- 최근에 각각 다른 NLP task를 위한 커스텀 모델이 빠르게 증가하였다. 커스텀하는 방법은 어떤것이 있을까?
- Transformer가 소개된 이후로, 아주 큰 모델이 출시되었다. 이들은 추가적인 레이블링이 필요없는, 방대한 데이터셋을 통해 자가지도학습(Self-supervised training)을 진행한다. 예를들어, BERT, GPT-3가 있다.
- 이후에 모델에 전이학습(transfer learning)이 적용되어 커스텀 모델이 증가했다.
- 최근에 이런 모델들이 NLP에 필수가 되면서, 방대한 데이터를 학습하는 것이 필수가 되었다. 따라서 NLP 연구는 한정된 GPU 자원으로 모델을 학습하기에는 무리가 있다는 단점을 가진다.

