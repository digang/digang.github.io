---
layout: post
title : "[ML 머신러닝] GAN - 정복하기 (3) GAN 의 종류"
categories: [ML, 머신러닝, GAN, 간]
tags: [gan, ml, 머신러닝, 간, 갠] # TAG 는 소문자로 작성할 것
use_math : true
---

# **Preview**

이번 포스트에서는 **GAN의 종류**에 대해 알아보는 시간을 가지겠다.

보통의 딥러닝 Network들과 같이 GAN또한 많은 종류의 형태와 종류들이 있다.

---

## **DCGAN(Deep Convolution GAN)**

**DCGAN**은 Deep Convolutional Generatice Adversarial Network의 약자로, GAN을 개선시키고 거기에 Convolution을 적용한 모델입니다.

출처: [미미로그 https://memesoo99.tistory.com/32](https://memesoo99.tistory.com/32)
> 대부분의 내용은 위 블로그를 참조하여 작성하였다. 본 블로그보다 훨~씬 자세한 설명이 있으니 깊게 공부하고 싶은 사람은 위 블로그를 참조하자!

Ian Goodfellow가 처음 제안한 적대적 생성 신경망(GAN)은 획기적이었으나 구조가 다소 불안정하고, NN이 기본적으로 지닌 Black Box 한계점 = '어떠한 과정을 거쳐서 이런 결과가 나오는거지?'에 대한 설명이 부족했다.

때문에 논문에서는 DCGAN 을 통하여 다음 내용들을 보완한다고 설명한다.

1. 거의 대부분의 상황에서 **안정적 학습**이 가능한 GAN인 DCGAN을 제시한다.
2. 학습이 된 판별기(이하 D)가 이미지 분류에서 다른 비지도 알고리즘들과 비교했을때 대등한 성능을 보인다.
3. <span class="custom_underline">DCGAN이 학습한 filter들을 visualize하고, 특정 filter가 특정 object를 생성하는 역할을 한다는것을 알아냈다.</span>
4. <span class="custom_underline">DCGAN이 벡터 산술 연산이 가능한 성질을 갖는다. Semantic quality를 갖는다.
</span>

여기서 주목해야할 부분은 3번과 4번의 특징이다. 아주 흥미로운 점이다. 

3번의 예를 들자면, 방(room)의 사진을 생성해내는 G가 있다고 해보자. 이때, 방이라면 당연히 창문이 존재할 것 이다. **3번의 특징이 여기서 발현된다.** G의 filter에서 창문을 생성해내는 특징을 색출하고 그 특징을 dropout 하면 특징이 사라지게 된다. 즉 창문이 없는 방을 생성해낼 수 있다.

![창이없는방](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcDr4BZ%2Fbtq8YVXuDjd%2F3NcMmOkgw0nJNUF6KJnusk%2Fimg.png)

4번의 내용은 글로 봐서는 어떠한 내용인지 잘 감이 오지 않을텐데, (벡터 산술 연산..? 그게 뭐지) 내가 이해한걸 토대로 쉽게 설명하자면 (웃는여자) - (여자의 특성) + (남자의 특성) = (웃는 남자) 의 사진을 생성해낼 수 있다는 의미이다!

![웃는남자](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNVGVR%2Fbtq8UPRjYtX%2FZQ4CW3TMzwENkhEYyEjivk%2Fimg.png)

방법은 이러하다.
1. 각 카테고리 (웃는여자), (일반적여자), (일반적남자) 의 z를 평균을 내어 Z vector 를 생성한다.
2. 평균값을 구한 각 Z vector들을 연산한다.
   > 위의 에시처럼 웃는여자 - 여자 + 남자 
3. 결과적으로 웃는 남자의 얼굴이 생성된다.

보다 자세한 내용이 알고 싶다면, 위의 링크를 참조하여 공부하도록 하자.

---

## **Least Squares GAN(LSGAN)**

![leastsquaresgan](https://mblogthumb-phinf.pstatic.net/MjAxOTA2MTBfMTY4/MDAxNTYwMTEzNTExODA1.1kb-s09cVF1nxs34LlSC-8hnn7bm_dukAIJInrw3Xx4g.in6MO12HYs443uJ7a0NFoF9xcIk3bind_Ym7Yq1VUz4g.PNG.euleekwon/image.png?type=w800)

- 먼저 (b)의 이미지를 볼 필요가 있다.
  - (b)의 이미지에서 파란색 선(Decision Boundary)를 기준으로 아래는 Real, 위는 Fake로 간주한다고 생각해보자.
  - 현재 **분홍색**으로 표시되어 있는 **별모양**에 집중해보면, 저 이미지들은 **Fake**임에도 불구하고, Boundary를 기준으로 아래에 있다는 이유로 Real이미지로 분류되어 있다.
  - 이때, G의 입장에서 생각해보면 저들은 이미 '진짜'로 분류되어지기 때문에 더이상 학습될 필요가 없다.
  - <span class="custom_underline">**이를 위해 Least suqare loss를 사용하여**</span> (기존의 GAN은 Binary cross entropy를 사용한다) 멀리 있는 sample에게 더 큰 패널티를 부여하여 저들을 좀더 Real image의 Boundary로 끌고 오자는게 목적이다.

- 그 결과 (a)에서 멀리있는 Fake Sample 들이 더 큰 패널티를 부여받음으로써, 좀 더 Real Image 의 Boundary로 끌고 와짐을 확인할 수 있다.

![least_entropy](https://blog.kakaocdn.net/dn/6xmL9/btq255SDkzF/Er2DshYlk1DDtd1a1mnJx0/img.gif)

![수식](https://blog.kakaocdn.net/dn/yFDwk/btq21oeP1PS/FnkhrsN66KlybpFWvTanN1/img.gif)

<span class="custom_underline_green">**이처럼, 거리가 멀수록 큰 패널티를 얻기 때문에, 가짜 이미지들은 거리를 좁히는 방향으로 새로 생성되게 된다.**</span>

#### **나머지 내용은 다음시간에~**

---
## **References**

[https://di-bigdata-study.tistory.com/12](https://di-bigdata-study.tistory.com/12) - 미미로그


