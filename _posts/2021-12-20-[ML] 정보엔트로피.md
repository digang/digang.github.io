---
layout: post
title : "[ML 머신러닝] 정보-엔트로피 (information-entropy)"
categories: [ML, 머신러닝, Information, 정보]
tags: [entropy, ml, 머신러닝, 정보, information] # TAG 는 소문자로 작성할 것
use_math : true
---

# **정보 엔트로피**

이번 시간에는 정보엔트로피에 관해 알아보겠다. 아마 처음 보는 사람은 정보에도 엔트로피가 존재하는가? 에 대해 의문을 가질 것이다. (나 또한 그랬으니까) 

**따라서 이번 시간에 자세히 알아보도록 하자!**

---

## **정보(information)**

정보에대해 알아보기 전에 우리가 알아볼 **정보(information)**는 단순히 통계학적 관점임을 알고 공부하도록 하자. 

가령 예를들면, 데이터베이스의 관점에서는 데이터를 쉽게 사용할 수 있도록 가공한것을 정보라 할 수 있다.

하지만 **통계학적** 관점이라면..?
- 통계학적 관점에서 **정보**는 사건(event)가 일어날 확률을 낮춰주는 일종의 증거(?)이다.
- <span class="custom_underline">즉 **정보가 많을수록 확률은 낮아진다**</span>
  - 여기에 대해 잘 이해가 안될 수 있는데 예를들어 누군가가 당신에게 **사막에 눈이 올 확률을 물어본다고 해보자** 당신은 몇 %의 확률을 예상할까?
  - 아마 당연히 5% 미만 혹은 아주 작은 수의 확률을 예상할 것 이다. 어째서일까?
  - **이유는 당신은 이미 사막에 관련된 간단한 지식 (덥다던지, 건조하다던지) 들을 (이것을 우리는 실제로도 정보라고 부르지 않는가? ㅎㅎ) 알고 있기 때문이다.**
  - 이러한 이유로 통계학의 관점에서 정보량이 많을수록, 확률은 낮게 예측된다고 본다.

때문에 이러한 수식이 성립한다.

$$
정보량 \propto \frac{1}{P(X)}
$\$

**즉 정보량은 확률에 반비례한다.**

---

## **정보량**

조금 더 구체적으로 하자면, 통계학에서의 정보량은 다음과 같이 정의한다.

랜덤변수 $X$에 대해서

$$
I(X) = -log_b (P(X))
$\$
> b는 2, e, 10중 하나를 의미한다.

굳이 log 를 붙인 이유는

1. 확률값(혹은 롹률밀도 값)에 **반비례**해야 하기 때문.
2. 두 사건의 정보량을 합치면 각 사건의 정보량을 합친 것과 같아야 함.

라고 할 수 있다.

---

## **엔트로피 (평균 정보량)**

통계학에서 엔트로피는 <span class="custom_underline">**평균 정보량**</span>을 의미한다고 한다.

> 열역학이나 다른 여러가지 역학을 배운 공학도들은 엔트로피가 평균 정보량이라는 것에 대해서 이해하지 못하겠지만 어쩔수없다. 누군가가 미리 이렇게 정의해놓은걸.. 그냥 받아들이자. 그냥 열역학에서 배운 엔트로피와는 관계가 **전혀** 없다고 한다 ^^.

따라서 이산 랜덤변수 $X$의 샘플 공간이 ${x_1,x_2,x_3,...,x_n}$ 이라고 할때,다음과 같이 정의된다. 
$$
H(X) = E[I(X)] = - \sum_{i=1}^n P(x_i)log_b (P(x_i))
$\$

여기서 $E$는 기댓값 연산자를 의미한다.

---
## **Reference**

[https://angeloyeo.github.io/2020/10/26/information_entropy.html](https://angeloyeo.github.io/2020/10/26/information_entropy.html) - 공돌이의 수학정리노트