---
layout: post
title : "[ML 머신러닝] Transformer 모델 기초"
categories: [ML, Sequential Model, Transformer]
tags: [머신러닝 , 딥러닝, ml , transformer] # TAG 는 소문자로 작성할 것
use_math : true
---

# **Transformer**

## **Sequential Model의 한계점**

`RNN`에서 다루었던 `Sequential Model`들은 완벽한 구성성분을 가진 특정 데이터가 아니면 학습하기가 어려웠다.

- 문장을 학습할 때
  - **Origianl Sequence**
    - 나는 오늘 학교에 가서 학식을 먹었다.
  - **Trimmed Sequence** - 문장마다 길이가 다르다.
    - 나는 오늘 학교에 갔다.
  - **Omitted Sequence** - 문장성분이 누락될 수 있음.
    - 오늘 학식 먹었다.
  - **Permuted Sequence** - 성분의 순서가 permute 될 수 있음.
    - 오늘 학교 가서 먹었지, 나는

<span class="custom_underline">**이러한 문제를 해결하기 위한 것이 Transformer 구조이다.**</span>

## **Transformer 구조?**

### **1. Transformer 란?**

트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 "Attention is all you need"에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델입니다. 이 모델은 RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여주었습니다.

### **2. Transformer 의 주요 하이퍼파라미터**
시작에 앞서 트랜스포머의 하이퍼파라미터를 정의합니다. 각 하이퍼파라미터의 의미에 대해서는 뒤에서 설명하기로하고, 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해해보겠습니다. 아래에서 정의하는 수치는 트랜스포머를 제안한 논문에서 사용한 수치로 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들입니다.

